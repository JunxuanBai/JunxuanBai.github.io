<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Junxuan Bai's Homepage (Beihang University) 白隽瑄 的主页</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Junxuan Bai</h1>
        <h2><b>白隽瑄</b></h2>
        <p>Ph.D Candidate of Computer Science<br><a href="http://ev.buaa.edu.cn/">Beihang University</a></p>
        <p>Research Affiliate<br><a href="http://vrlab.buaa.edu.cn/index.htm">State Key Laboratory of Virtual Reality Technology and System</a></p>
    <h3><p class="view"><a href="https://junxuanbai.github.io/">Home</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/research.html">Research & Publications</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/code.html">Code</a></p></h3>
<!--     <h3><p class="view"><a href="https://junxuanbai.github.io/research/CV.pdf">CV</a></p></h3>   -->
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/code.html">Code</a></p></h3>  -->
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/project.html">Project</a></p></h3>  -->
        <h3><p class="view"><a href="https://junxuanbai.github.io/education.html">Education</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/award.html">Award</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/usefullinks.html">Useful Links</a></p></h3>
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/personal.html">Personal</a></p></h3> -->
    <p class="view"><b>Social</b><br>
        <a href="mailto:baijx6@163.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
        <a href="http://github.com/junxuanbai"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>
        <a href="https://dblp.dagstuhl.de/pid/153/5720.html"> DBLP</a><br>
      <div itemscope itemtype="https://schema.org/Person"><a itemprop="sameAs" content="https://orcid.org/0000-0002-7941-0584" href="https://orcid.org/0000-0002-7941-0584" target="orcid.widget" rel="me noopener noreferrer" style="vertical-align:top;"><img src="https://orcid.org/sites/default/files/images/orcid_16x16.png" style="width:1em;margin-right:.5em;" alt="ORCID iD icon">https://orcid.org/0000-0002-7941-0584</a></div><br>


    <p><b>Contact:</b><br>New Main Building G714<br>Beihang University<br>Xueyuan Road 37<br>Haidian District, Beijing 100191</p>
      </header>
      <section>

    <h2><a id="Journal_papers" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Journal Papers</h2>
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">Diverse Dance Synthesis via Keyframes with Transformer Controllers</a> 
        <br> Junjun Pan, Siyuan Wang, <b>Junxuan Bai</b>, Ju Dai
        <br> <i>Computer Graphics Forum</i>, Special Issue--Pacific Graphics 2021, 2021.
        <br> <a href="https://github.com/godzillalla/Dance-Synthesis-Project">[code]</a>
    </p>
        
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://onlinelibrary.wiley.com/doi/10.1002/cav.1996">EmoDescriptor: A hybrid feature for emotional classification in dance movements</a> 
        <br> <b>Junxuan Bai</b>, Rong Dai, Ju Dai, and Junjun Pan
        <br> <i>Computer Animation and Virtual Worlds</i>, Early View, 2021.
        <br> <a href="https://drive.google.com/file/d/1o-tgxsIUHDtTUs5yYsjtN2qLwHNliJwX/view?usp=sharing">[pdf]</a> <a href="https://drive.google.com/file/d/1BY8P-44bhB0DLnYNNElcxaLjUCgYk0XN/view?usp=sharing">[demo]</a>
    <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Similar to language and music, dance performances provide an effective way to express human emotions. 
      With the abundance of the motion capture data, content‐based motion retrieval and classification have been fiercely investigated. 
      Although researchers attempt to interpret body language in terms of human emotions, the progress is limited by the scarce 3D motion database annotated with emotion labels. 
      This article proposes a hybrid feature for emotional classification in dance performances. 
      The hybrid feature is composed of an explicit feature and a deep feature. 
      The explicit feature is calculated based on the Laban movement analysis, which considers the body, effort, shape, and space properties. 
      The deep feature is obtained from latent representation through a 1D convolutional autoencoder. 
      Eventually, we present an elaborate feature fusion network to attain the hybrid feature that is almost linearly separable. 
      The abundant experiments demonstrate that our hybrid feature is superior to the separate features for the emotional classification in dance performances.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2021_CAVW_EmoDescriptor.png">
        
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://link.springer.com/article/10.1007/s00371-019-01678-7">Interactive animation generation of virtual characters using single RGB-D camera</a> 
        <br> Ning Kang, <b>Junxuan Bai</b> (co-first author), Junjun Pan, and Hong Qin
        <br> <i>The Visual Computer</i>, 35(6-8): 849-860, 2019.
        <br> <a href="https://drive.google.com/file/d/1NGLk3u8yjp6WNcPvimdkM5OkzXDnNY4L/view?usp=sharing">[pdf]</a> <a href="https://drive.google.com/file/d/1iBKCEjrOlKvWWgJmTHRXM8p1gad_cGby/view">[presentation]</a>
    <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The rapid creation of 3D character animation by commodity devices plays an important role in enriching visual content in virtual reality. This paper concentrates on addressing the challenges of current motion imitation for human body. We develop an interactive framework for stable motion capturing and animation generation based on single Kinect device. In particular, we focus our research efforts on two cases: (1) The participant is facing the camera; or (2) the participant is turning around or is side facing the camera. Using existing methods, camera could obtain a proﬁle view of the body, but it frequently leads to less satisfactory result or even failure due to occlusion. In order to reduce certain artifacts appeared at the side view, we design a mechanism to reﬁne the movement of the human body by integrating an adaptive ﬁlter. After specifying the corresponding joints between the participant and the virtual character, the captured motion could be retargeted in a quaternion-based manner. To further improve the animation quality, inverse kinematics are brought into our framework to constrain the target’s positions. A large variety of motions and characters have been tested to validate the performance of our framework. Through experiments, it shows that our method could be applied to real-time applications, such as physical therapy and ﬁtness training.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2019_TVC_InteractiveAnimationGeneration.png">
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://link.springer.com/article/10.1007%2Fs11432-018-9470-4">Novel metaballs-driven approach with dynamic constraints for character articulation</a> 
        <br> <b>Junxuan Bai</b>, Junjun Pan, Yuhan Yang, and Hong Qin
        <br> <i>SCIENCE CHINA Information Science</i>, 61(9): 094101:1-094101:3, 2018.
        <br> <a href="http://scis.scichina.com/en/2018/094101.pdf">[pdf]</a> <a href="http://scis.scichina.com/en/2018/094101-ppt.pdf">[slides]</a> <a href="http://scis.scichina.com/en/2018/094101.html">[demo]</a>
    <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Skinning techniques are essential for character articulation in 3D computer animation. Currently, skeleton-based methods are widely used in the animation industry for its simplicity and efficiency, especially in linear blend skinning (LBS) and dual quaternion skinning (DQS). However, owing to the lack of the inside volumetric representation, they suffer from joint collapse, candy-wrapper, and bulging problems.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2018_SCIS_MetaballsSkinning.png">  
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://onlinelibrary.wiley.com/doi/full/10.1002/cav.1724">Essential techniques for laparoscopic surgery simulation</a> 
        <br> Kun Qian, <b>Junxuan Bai</b>, Xiaosong Yang, Junjun Pan, Jian-Jun Zhang
        <br> <i>Computer Animation and Virtual Worlds</i>, 28(2), 2017.
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Laparoscopic surgery is a complex minimum invasive operation that requires long learning curve for the new trainees to have adequate experience to become a qualified surgeon. With the development of virtual reality technology, virtual reality‐based surgery simulation is playing an increasingly important role in the surgery training. The simulation of laparoscopic surgery is challenging because it involves large non‐linear soft tissue deformation, frequent surgical tool interaction and complex anatomical environment. Current researches mostly focus on very specific topics (such as deformation and collision detection) rather than a consistent and efficient framework. The direct use of the existing methods cannot achieve high visual/haptic quality and a satisfactory refreshing rate at the same time, especially for complex surgery simulation. In this paper, we proposed a set of tailored key technologies for laparoscopic surgery simulation, ranging from the simulation of soft tissues with different properties, to the interactions between surgical tools and soft tissues to the rendering of complex anatomical environment. Compared with the current methods, our tailored algorithms aimed at improving the performance from accuracy, stability and efficiency perspectives. We also abstract and design a set of intuitive parameters that can provide developers with high flexibility to develop their own simulators.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2017_CAVW_EssentialTechniques.png">
  
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://onlinelibrary.wiley.com/doi/full/10.1002/cav.1655">Real-time haptic manipulation and cutting of hybrid soft tissue models by extended position-based dynamics</a> 
        <br> Junjun Pan, <b>Junxuan Bai</b>, Xin Zhao, Aimin Hao, and Hong Qin
        <br> <i>Computer Animation and Virtual Worlds</i>, 26(3-4): 321-335, 2015.
        <br> <a href="https://drive.google.com/file/d/1jdgVL7HmfQIk8h0BbR1PkwRjlzsRNX8_/view?usp=sharing">[pdf]</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper systematically describes an interactive dissection approach for hybrid soft tissue models governed by extended position‐based dynamics. Our framework makes use of a hybrid geometric model comprising both surface and volumetric meshes. The fine surface triangular mesh with high‐precision geometric structure and texture at the detailed level is employed to represent the exterior structure of soft tissue models. Meanwhile, the interior structure of soft tissues is constructed by coarser tetrahedral mesh, which is also employed as physical model participating in dynamic simulation. The less details of interior structure can effectively reduce the computational cost during simulation. For physical deformation, we design and implement an extended position‐based dynamics approach that supports topology modification and material heterogeneities of soft tissue. Besides stretching and volume conservation constraints, it enforces the energy preserving constraints, which take the different spring stiffness of material into account and improve the visual performance of soft tissue deformation. Furthermore, we develop mechanical modeling of dissection behavior and analyze the system stability. The experimental results have shown that our approach affords real‐time and robust cutting without sacrificing realistic visual performance. Our novel dissection technique has already been integrated into a virtual reality‐based laparoscopic surgery simulator.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2015_CAVW_Real-timeHapticManipulation.png">  

    <hr>

    <h2><a id="Conference_papers" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conference Papers</h2>

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">3D-CariNet: End-to-end 3D caricature generation from natural face images with differentiable renderer</a> 
        <br> Meijia Huang, Ju Dai, Junjun Pan, <b>Junxuan Bai</b>, Hong Qin
        <br> <i>Pacific Graphics 2021</i>, Short paper, 2021
    </p>
      
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">Human motion synthesis and control via contextual manifold embedding</a> 
        <br> Rui Zeng, Ju Dai, <b>Junxuan Bai</b>, Junjun Pan, Hong Qin
        <br> <i>Pacific Graphics 2021</i>, Short paper, 2021
    </p>
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://drive.google.com/file/d/1-O5p5d0RLW1vdb-TutlQvKrsGEzR4n1Q/view">Flower factory: a component-based approach for rapid flower modeling</a> 
        <br> Siyuan Wang, Junjun Pan, <b>Junxuan Bai</b>, Jinglei Wang
        <br> <i>ISMAR 2020</i>: 12-23, Conference paper, 2020
        <br> <a href="https://drive.google.com/file/d/1XeXN7CmxzDarOJtU7fiZmZu86AvUXdfG/view?usp=sharing">[pdf]</a> <a href="https://youtu.be/9DpNJr49HGs">[presentation]</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> 
      The rapid 3D objects modeling provides an effective way to enrich digital content, which is one of the essential tasks in VR/AR research. 
      Flowers are frequently utilized in real-time applications, such as video games and VR/AR scenes. 
      Technically, a realistic flower generation using the existing 3D modeling software is complicated and time-consuming for designers. 
      Moreover, it is difficult to create imaginary and surreal flowers, which might be more interesting and attractive for the artists and game players. 
      In this paper, we propose a component-based framework for rapid flower modeling, called Flower Factory. 
      The flowers are assembled by different components, e.g., petals, stamens, receptacles and leaves. 
      The shape of these components are created using simple primitives such as points and splines. 
      After the shape of models are determined, the textures are synthesized automatically based on a predefine mask, according to a number of rules from real flowers. 
      The whole modeling process can be controlled by several parameters, which describe the physical attributes of the flowers. 
      Our technique is capable of producing a variety of flowers rapidly. Even novices without any modeling skills are able to control and model the 3D flowers. 
      Furthermore, the developed system will be integrated in a lightweight application of smartphone due to its low computational cost.  </div></p>
    <img src="https://JunxuanBai.github.io/research/ISMAR2020_representative_image.png">

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://ieeexplore.ieee.org/document/8797856">Real-time animation and motion retargeting of virtual characters based on single RGB-D camera</a> 
        <br> Ning Kang, <b>Junxuan Bai</b>, Junjun Pan, Hong Qin
        <br> <i>VR 2019</i>: 1006-1007, Poster, 2019
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The rapid generation and flexible reuse of characters animation by commodity devices are of significant importance to rich digital content production in virtual reality. This paper aims to handle the challenges of current motion imitation for human body in several indoor scenes (e.g., fitness training). We develop a real-time system based on single Kinect device, which is able to capture stable human motions and retarget to virtual characters. A large variety of motions and characters are tested to validate the efficiency and effectiveness of our system.  </div></p>

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://dl.acm.org/doi/10.1145/2821592.2821599">Virtual reality based laparoscopic surgery simulation</a> 
        <br> Kun Qian, <b>Junxuan Bai</b>, Xiaosong Yang, Junjun Pan, Jian-Jun Zhang
        <br> <i>VRST 2015</i>: 321-335, Conference paper, 2015
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> With the development of computer graphic and haptic devices, training surgeons with virtual reality technology has proven to be very effective in surgery simulation. Many successful simulators have been deployed for training medical students. However, due to the various unsolved technical issues, the laparoscopic surgery simulation has not been widely used. Such issues include modeling of complex anatomy structure, large soft tissue deformation, frequent surgical tools interactions, and the rendering of complex material under the illumination of headlight. A successful laparoscopic surgery simulator should integrate all these required components in a balanced and efficient manner to achieve both visual/haptic quality and a satisfactory refreshing rate. In this paper, we propose an efficient framework integrating a set of specially tailored and designed techniques, ranging from deformation simulation, collision detection, soft tissue dissection and rendering. We optimize all the components based on the actual requirement of laparoscopic surgery in order to achieve an improved overall performance of fidelity and responding speed.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2015_VRST_VirtualRealityBasedSurgery.png">  

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://dl.acm.org/doi/abs/10.1145/2671015.2671129">Dissection of hybrid soft tissue models using position-based dynamics</a> 
        <br> Junjun Pan, <b>Junxuan Bai</b>, Xin Zhao, Aimin Hao, and Hong Qin
        <br> <i>VRST 2014</i>: 219-220, Poster, 2014
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper describes an interactive dissection approach for hybrid soft tissue models governed by position-based dynamics. Our framework makes use of a hybrid geometric model comprising both surface and volumetric meshes. The fine surface triangular mesh is used to represent the exterior structure of soft tissue models. Meanwhile, the interior structure of soft tissues is constructed by coarser tetrahedral meshes, which are also employed as physical models participating in dynamic simulation. The less details of interior structure can effectively reduce the computational cost of deformation and geometric subdivision during dissection. For physical deformation, we design and implement a position-based dynamics approach that supports topology modification and enforces the volume-preserving constraint. Experimental results have shown that, this hybrid dissection method affords real-time and robust cutting simulation without sacrificing realistic visual performance.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2014_VRST_DissectionUsingPBD.png">  

    <hr>

    <h2><a id="Engineering" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Engineering</h2>
    <!--<hr><a id="Other_work" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Work</h2>-->
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">Displaying platform for crude oil logging software</a> 
        <br> Zhen Wang, <b>Junxuan Bai</b>, Jingxue Li
        <br> China Oilfield Services Limited (COSL), 2012-2013
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The goal of this project is to visualize logging data on a pad computer.  </div></p>
    <img src="https://JunxuanBai.github.io/research/DisplayPlatform.jpg">    
    


     </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
