<!doctype html>
<html>
  <head>
  <script src="https://use.fontawesome.com/baff6f55f5.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Junxuan Bai's Homepage (Beihang University)</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-29643011-3', 'auto');
      ga('send', 'pageview');
    </script>

    <!-- For all browsers -->
    <link rel="stylesheet" href="assets/css/academicons.min.css"/>
    <link rel="stylesheet" href="assets/css/academicons.css"/>
    
    <style>
      button.accordion {
      font:14px/1.5 Lato, "Helvetica Neue", Helvetica, Arial, sans-serif;
      cursor: pointer;
      padding: 0px;
      border: none;
      text-align: left;
      outline: none;
      font-size: 100%;
      transition: 0.3s;
      background-color: #f8f8f8;
      }
      button.accordion.active, button.accordion:hover {
      background-color: #f8f8f8;
      }
      button.accordion:after {
      content: " [+] ";
      font-size: 90%;
      color:#777;
      float: left;
      margin-left: 1px;
      }

      button.accordion.active:after {
      content: " [\2212] ";
      }
      div.panel {
      padding: 0 20px;
      margin-top: 5px;
      display: none;
      background-color: white;
      font-size: 100%;
      }
      div.panel.show {
      display: block !important;
      }
    </style>
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Junxuan Bai</h1>
        <p>Ph.D Candidate of Computer Science<br><a href="http://ev.buaa.edu.cn/">Beihang University</a></p>
        <p>Research Affiliate<br><a href="http://vrlab.buaa.edu.cn/index.htm">State Key Laboratory of Virtual Reality Technology and System</a></p>
    <h3><p class="view"><a href="https://junxuanbai.github.io/">Home</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/research.html">Research</a></p></h3>
<!--     <h3><p class="view"><a href="https://junxuanbai.github.io/research/CV.pdf">CV</a></p></h3>   -->
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/code.html">Code</a></p></h3>  -->
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/project.html">Project</a></p></h3>  -->
        <h3><p class="view"><a href="https://junxuanbai.github.io/education.html">Education</a></p></h3>
        <h3><p class="view"><a href="https://junxuanbai.github.io/award.html">Award</a></p></h3>
<!--         <h3><p class="view"><a href="https://junxuanbai.github.io/personal.html">Personal</a></p></h3> -->
    <p class="view"><b>Social</b><br>
        <a href="mailto:baijx6@163.com" class="author-social" target="_blank"><i class="fa fa-fw fa-envelope-square"></i> Email</a><br>
        <a href="http://github.com/junxuanbai"><i class="fa fa-fw fa-github-square"></i> GitHub</a><br>

    <p><b>Contact:</b><br>New Main Building G714<br>Beihang University<br>Xueyuan Road 37<br>Haidian District, Beijing 100191</p>
      </header>
      <section>

    <h2><a id="Journal_papers" class="anchor" href="#publications" aria-hidden="true"><span class="octicon octicon-link"></span></a>Journal Papers</h2>
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2019_TVC_InteractiveAnimationGeneration.pdf">Interactive animation generation of virtual characters using single RGB-D camera</a> 
        <br> with Ning Kang, <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, and <a href="https://www3.cs.stonybrook.edu/~qin/">Hong Qin </a>
        <br> <i>The Visual Computer</i>, 35(6-8): 849-860 (2019).
        <br> <a href="https://drive.google.com/file/d/1iBKCEjrOlKvWWgJmTHRXM8p1gad_cGby/view">Presentation of CGI 2019</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The rapid creation of 3D character animation by commodity devices plays an important role in enriching visual content in virtual reality. This paper concentrates on addressing the challenges of current motion imitation for human body. We develop an interactive framework for stable motion capturing and animation generation based on single Kinect device. In particular, we focus our research efforts on two cases: (1) The participant is facing the camera; or (2) the participant is turning around or is side facing the camera. Using existing methods, camera could obtain a proﬁle view of the body, but it frequently leads to less satisfactory result or even failure due to occlusion. In order to reduce certain artifacts appeared at the side view, we design a mechanism to reﬁne the movement of the human body by integrating an adaptive ﬁlter. After specifying the corresponding joints between the participant and the virtual character, the captured motion could be retargeted in a quaternion-based manner. To further improve the animation quality, inverse kinematics are brought into our framework to constrain the target’s positions. A large variety of motions and characters have been tested to validate the performance of our framework. Through experiments, it shows that our method could be applied to real-time applications, such as physical therapy and ﬁtness training.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2019_TVC_InteractiveAnimationGeneration.png">
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2018_SCIS_MetaballsSkinning.pdf">Novel metaballs-driven approach with dynamic constraints for character articulation</a> 
        <br> with <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, Yuhan Yang, and <a href="https://www3.cs.stonybrook.edu/~qin/">Hong Qin </a>
        <br> <i>SCIENCE CHINA Information Science</i>, 61(9): 094101:1-094101:3 (2018).
        <br> <a href="http://scis.scichina.com/en/2018/094101.html">Multimedia resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Skinning techniques are essential for character articulation in 3D computer animation. Currently, skeleton-based methods are widely used in the animation industry for its simplicity and efficiency, especially in linear blend skinning (LBS) and dual quaternion skinning (DQS). However, owing to the lack of the inside volumetric representation, they suffer from joint collapse, candy-wrapper, and bulging problems.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2018_SCIS_MetaballsSkinning.png">  
    
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2017_CAVW_EssentialTechniques.pdf">Essential techniques for laparoscopic surgery simulation</a> 
        <br> with <a href="https://www.kunqiancg.com/bio">Kun Qian</a>, <a href="https://staffprofiles.bournemouth.ac.uk/display/xyang">Xiaosong Yang</a>, <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, <a href="https://staffprofiles.bournemouth.ac.uk/display/jjunzhang">Jian-Jun Zhang</a>
        <br> <i>Computer Animation and Virtual Worlds</i>, 28(2) (2017).
        <br> <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/cav.1724">Online resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> Laparoscopic surgery is a complex minimum invasive operation that requires long learning curve for the new trainees to have adequate experience to become a qualified surgeon. With the development of virtual reality technology, virtual reality‐based surgery simulation is playing an increasingly important role in the surgery training. The simulation of laparoscopic surgery is challenging because it involves large non‐linear soft tissue deformation, frequent surgical tool interaction and complex anatomical environment. Current researches mostly focus on very specific topics (such as deformation and collision detection) rather than a consistent and efficient framework. The direct use of the existing methods cannot achieve high visual/haptic quality and a satisfactory refreshing rate at the same time, especially for complex surgery simulation. In this paper, we proposed a set of tailored key technologies for laparoscopic surgery simulation, ranging from the simulation of soft tissues with different properties, to the interactions between surgical tools and soft tissues to the rendering of complex anatomical environment. Compared with the current methods, our tailored algorithms aimed at improving the performance from accuracy, stability and efficiency perspectives. We also abstract and design a set of intuitive parameters that can provide developers with high flexibility to develop their own simulators.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2017_CAVW_EssentialTechniques.png">
  
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2015_CAVW_Real-timeHapticManipulation.pdf">Real-time haptic manipulation and cutting of hybrid soft tissue models by extended position-based dynamics</a> 
        <br> with <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, Xin Zhao, Aimin Hao, and <a href="https://www3.cs.stonybrook.edu/~qin/">Hong Qin </a>
        <br> <i>Computer Animation and Virtual Worlds</i>, 26(3-4): 321-335 (2015).
        <br> <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/cav.1655">Online resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper systematically describes an interactive dissection approach for hybrid soft tissue models governed by extended position‐based dynamics. Our framework makes use of a hybrid geometric model comprising both surface and volumetric meshes. The fine surface triangular mesh with high‐precision geometric structure and texture at the detailed level is employed to represent the exterior structure of soft tissue models. Meanwhile, the interior structure of soft tissues is constructed by coarser tetrahedral mesh, which is also employed as physical model participating in dynamic simulation. The less details of interior structure can effectively reduce the computational cost during simulation. For physical deformation, we design and implement an extended position‐based dynamics approach that supports topology modification and material heterogeneities of soft tissue. Besides stretching and volume conservation constraints, it enforces the energy preserving constraints, which take the different spring stiffness of material into account and improve the visual performance of soft tissue deformation. Furthermore, we develop mechanical modeling of dissection behavior and analyze the system stability. The experimental results have shown that our approach affords real‐time and robust cutting without sacrificing realistic visual performance. Our novel dissection technique has already been integrated into a virtual reality‐based laparoscopic surgery simulator.  </div></p>  
    <img src="https://JunxuanBai.github.io/research/2015_CAVW_Real-timeHapticManipulation.png">  

    <hr>

    <h2><a id="Conference_papers" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conference Papers</h2>
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2019_VR_Real-timeRGBDAnimation.pdf">Real-time animation and motion retargeting of virtual characters based on single RGB-D camera</a> 
        <br> with Ning Kang, <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, <a href="https://www3.cs.stonybrook.edu/~qin/">Hong Qin </a>
        <br> <i>2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</i>: 1006-1007. (Poster)
        <br> <a href="https://ieeexplore.ieee.org/document/8797856">Online resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The rapid generation and flexible reuse of characters animation by commodity devices are of significant importance to rich digital content production in virtual reality. This paper aims to handle the challenges of current motion imitation for human body in several indoor scenes (e.g., fitness training). We develop a real-time system based on single Kinect device, which is able to capture stable human motions and retarget to virtual characters. A large variety of motions and characters are tested to validate the efficiency and effectiveness of our system.  </div></p>

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2015_VRST_VirtualRealityBasedSurgery.pdf">Virtual reality based laparoscopic surgery simulation</a> 
        <br> with <a href="https://www.kunqiancg.com/bio">Kun Qian</a>, <a href="https://staffprofiles.bournemouth.ac.uk/display/xyang">Xiaosong Yang</a>, <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, <a href="https://staffprofiles.bournemouth.ac.uk/display/jjunzhang">Jian-Jun Zhang</a>
        <br> <i>VRST 2015</i>: 321-335.
        <br> <a href="https://dl.acm.org/citation.cfm?id=2821599">Online resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> With the development of computer graphic and haptic devices, training surgeons with virtual reality technology has proven to be very effective in surgery simulation. Many successful simulators have been deployed for training medical students. However, due to the various unsolved technical issues, the laparoscopic surgery simulation has not been widely used. Such issues include modeling of complex anatomy structure, large soft tissue deformation, frequent surgical tools interactions, and the rendering of complex material under the illumination of headlight. A successful laparoscopic surgery simulator should integrate all these required components in a balanced and efficient manner to achieve both visual/haptic quality and a satisfactory refreshing rate. In this paper, we propose an efficient framework integrating a set of specially tailored and designed techniques, ranging from deformation simulation, collision detection, soft tissue dissection and rendering. We optimize all the components based on the actual requirement of laparoscopic surgery in order to achieve an improved overall performance of fidelity and responding speed.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2015_VRST_VirtualRealityBasedSurgery.png">  

    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold" href="https://JunxuanBai.github.io/research/2014_VRST_DissectionUsingPBD.pdf">Dissection of hybrid soft tissue models using position-based dynamics</a> 
        <br> with <a href="http://shi.buaa.edu.cn/junjun_pan/en/zdylm/29596/list/index.htm">Junjun Pan</a>, Xin Zhao, Aimin Hao, and <a href="https://www3.cs.stonybrook.edu/~qin/">Hong Qin </a>
        <br> <i>VRST 2014</i>: 219-220. (Poster)
        <br> <a href="https://dl.acm.org/citation.cfm?id=2671129">Online resources</a>
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> This paper describes an interactive dissection approach for hybrid soft tissue models governed by position-based dynamics. Our framework makes use of a hybrid geometric model comprising both surface and volumetric meshes. The fine surface triangular mesh is used to represent the exterior structure of soft tissue models. Meanwhile, the interior structure of soft tissues is constructed by coarser tetrahedral meshes, which are also employed as physical models participating in dynamic simulation. The less details of interior structure can effectively reduce the computational cost of deformation and geometric subdivision during dissection. For physical deformation, we design and implement a position-based dynamics approach that supports topology modification and enforces the volume-preserving constraint. Experimental results have shown that, this hybrid dissection method affords real-time and robust cutting simulation without sacrificing realistic visual performance.  </div></p>
    <img src="https://JunxuanBai.github.io/research/2014_VRST_DissectionUsingPBD.png">  

    <hr><a id="Other_work" class="anchor" href="#RRpapers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other Work</h2>
    <p style="margin:0;" }><p > <a style="margin:0; font-size:100%; font-weight:bold">Displaying platform for crude oil logging software</a> 
        <br> with Jingxue Li, Zhen Wang, and Aimin Hao
        <br> China Oilfield Services Limited (COSL), 2012-2013
        <button class="accordion">
      Abstract
    </button>
    </p>
    <div class="panel" style="background-color: #F1F1F1; color: #666; padding: 10px;"> The goal of this project is to visualize logging data on a pad computer.  </div></p>
    <img src="https://JunxuanBai.github.io/research/DisplayPlatform.jpg">    
    


     </section>
      <footer>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    <script> 
    var acc = document.getElementsByClassName("accordion");
    var i;

    for (i = 0; i < acc.length; i++) {
        acc[i].onclick = function(){
            this.classList.toggle("active");
            this.parentNode.nextElementSibling.classList.toggle("show");
      }
    }
    </script>
  </body>
</html>
